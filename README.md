<div align="center">
  <picture>
    <source srcset="https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/Xiaomi_MiMo_darkmode.png?raw=true" media="(prefers-color-scheme: dark)">
    <img src="https://github.com/Xiaomi-Embodied/Xiaomi-Embodied/blob/main/figures/Xiaomi-Embodied.jpg?raw=true" width="60%" alt="Xiaomi-Embodied" />
  </picture>
</div>

<h3 align="center">
  <b>
    <span>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>
    <br/>
    Xiaomi-Embodied Technical Report
    <br/>
    <span>â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”</span>
    <br/>
  </b>
</h3>

<br/>

<div align="center" style="line-height: 1;">
  |
  <a href="https://huggingface.co/collections/XiaomiMiMo/mimo-vl-68382ccacc7c2875500cd212" target="_blank">ğŸ¤— HuggingFace</a>
  &nbsp;|
  <a href="https://www.modelscope.cn/collections/MiMo-VL-bb651017e02742" target="_blank">ğŸ¤–ï¸ ModelScope</a>
  &nbsp;|
  <a href="https://arxiv.org/abs/2506.03569" target="_blank">ğŸ“” Technical Report</a>
  &nbsp;|
  <a href="https://github.com/XiaomiMiMo/lmms-eval" target="_blank">ğŸ“Š Evaluation Framework</a>
  &nbsp;|
  <a href="./demo/README.md" target="_blank">ğŸ”¥ Gradio Demo</a>
  &nbsp;|
  <br/>
</div>

<br/>

# **Xiaomi-Embodied**

### ğŸ”¥ğŸ”¥ğŸ”¥ MiMo-VL 2508 Updates

We're excited to announce improvements to our MiMo-VL ([MiMo-VL-7B-RL-2508](https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL-2508) and [MiMo-VL-7B-SFT-2508](https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-SFT-2508)), featuring enhanced performance across multiple benchmarks, improved thinking control capabilities, and better user experience.

#### ğŸ“ˆ Performance Improvements

MiMo-VL-7B-RL-2508 demonstrates consistent improvements across both image and video benchmarks, achieving notable milestones of **70.6 on MMMU** and **70.8 on VideoMME**.

<img src="./figures/mimo-2508.png" alt="Benchmark Improvements" width="768">


Full evaluation results can be found [below](#full-evaluation-results).

#### ğŸ¤” Thinking Control Feature

A thinking control capability that allows users to turn off the model's reasoning mode using the no_think parameter:
- Thinking mode (default behavior): Full reasoning process visible with 100% control success rate;
- Non-thinking mode: Direct responses without reasoning, with a 99.84% control success rate
  
#### â¤ï¸ Enhanced User Experience

Our internal VLM Arena ratings show meaningful improvement in real-world performance:
- Current model (MiMo-VL-7B-RL-2508): 1131.2 rating
- Previous version (MiMo-VL-7B-RL): 1093.9 rating

These updates deliver a more capable, flexible, and reliable vision-language model for both academic evaluation and practical applications.


#### ğŸ˜‰ Case Study: What are the appealing features of this car?

![Xiaomi SU7](./figures/su7.gif)

**MiMo-VL-7B-RL-2508's Answer**:  

```markdown
<think>...</think>
To determine the appealing features of the Xiaomi SU7, we analyze visual elements and context from the video:
1. Aesthetic Design
- The car boasts a sleek, aerodynamic exterior with smooth, modern lines.
- Distinctive lighting (sharp headlights and a continuous, glowing taillight strip) enhances its futuristic and sporty look.
- Vibrant color and polished finish make it visually striking.
2. Performance & Speed
- Multiple dynamic shots of the car driving at high speeds on diverse roads (desert, forest, city) emphasize agility and power.
- Close - ups of spinning wheels and the engine (with lightning-like effects) suggest advanced engineering and high-performance capabilities (likely electric or tech-driven).
3. Versatility
- The car is shown in varied environments (desert at dusk, forested roads, urban streets), highlighting its adaptability to different terrains and use cases.
4. Technological Innovation
- The engine close-up with electric-like lightning effects implies cutting-edge technology, positioning the car as a tech-forward vehicle (aligning with Xiaomiâ€™s tech brand identity).
5. Brand Appeal
Prominent â€œXiaomi SU7â€ branding leverages Xiaomiâ€™s reputation in consumer tech, adding credibility and allure for tech-savvy audiences.


In summary, the Xiaomi SU7 appeals through its striking design, high-performance capabilities, versatility, technological innovation, and strong brand association.
```

#### Full Evaluation Results
<img src="./figures/metrics.jpeg" alt="Evaluation Results" width="768">

#### Model Recommendation

Both versions of the MiMo-VL-7B-2508 model are now open-sourced on Hugging Face:  
- [ğŸ¤— **MiMo-VL-7B-RL-2508**](https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL-2508 )
  - Recommended for most users to experience and utilize.  
- [ğŸ¤— **MiMo-VL-7B-SFT-2508**](https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-SFT-2508  )
  - Users may perform SFT and RL based on this model. Compared to the previous SFT version, this model demonstrates higher RL stability.  

#### Deployment Parameters
- temperature=0.3, topp=0.95  
- The system prompt is already set in `chat_template.json` and does not require additional configuration.
  


#### Thinking Control

Users can control the thinking mode by appending `/no_think` to queries:  
- **Thinking mode query** (default):  
  *"What is the answer to the question in the image?"*  
- **Non-thinking mode query**:  
  *"Identify the text in the image. /no_think"*. 

â—ï¸Important: The `/no_think` command must be the very last part of user message, which means after `/no_think`, there shouldn't be any user content like image or video.
  
#### Placing Visual Input
For prompts with a single image or video, always place the visual media before the text. For example:

âœ… Good:
```
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image_path},
            {"type": "text",  "text": "Describe the image. /no_think"},
        ],
    }
]
```

âŒ Bad:
```
messages = [
    {
        "role": "user",
        "content": [
            {"type": "text",  "text": "Describe the image. /no_think"},
            {"type": "image", "image": image_path},
        ],
    }
]
```

---


## I. Introduction

In this report, we share our efforts to build a compact yet powerful VLM, MiMo-VL-7B. MiMo-VL-7B comprises (1) a native resolution ViT encoder that preserves fine-grained visual details, (2) an MLP projector for efficient cross-modal alignment, and (3) our [MiMo-7B language model](https://github.com/XiaomiMiMo/MiMo), specifically optimized for complex reasoning tasks. 

The development of MiMo-VL-7B involves two sequential training processes: (1) A four-stage pre-training phase, which includes projector warmup, vision-language alignment, general multi-modal pre-training, and long-context Supervised Fine-Tuning (SFT). This phase yields the MiMo-VL-7B-SFT model. (2) A subsequent post-training phase, where we introduce Mixed On-policy Reinforcement Learning (MORL), a novel framework that seamlessly integrates diverse reward signals spanning perception accuracy, visual grounding precision, logical reasoning capabilities, and human/AI preferences. This phase yields the MiMo-VL-7B-RL model.

<p align="center">
  <img width="95%" src="https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/benchmarks.png?raw=true">
</p>

We open-source MiMo-VL-7B series, including checkpoints of the SFT and RL model.
We believe this report along with the models will provide valuable insights to develop powerful reasoning VLMs that benefit the larger community.

### ğŸ›¤ï¸ During this journey, we find

- **Incorporating high-quality, broad-coverage reasoning data from the pre-training stage is crucial for enhancing model performance**
  - We curate high-quality reasoning data by identifying diverse queries, employing large reasoning models to regenerate responses with long CoT, and applying rejection sampling to ensure quality.
  - Rather than treating this as supplementary fine-tuning data, we incorporate substantial volumes of this synthetic reasoning data directly into the later pre-training stages, where extended training yields continued performance improvements without saturation.
- **Mixed On-policy Reinforcement Learning further enhances model performance, while achieving stable simultaneous improvements remains challenging**
  - We apply RL across diverse capabilities, including reasoning, perception, grounding, and human preference alignment, spanning modalities including text, images, and videos. While this hybrid training approach further unlock modelâ€™s potential, interference across data domains remains a challenge.

## II. Model Details

<p align="center">
  <img width="95%" src="https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/architecture.png?raw=true">
</p>

> Models are available at [Huggingface Collections: MiMo-VL](https://huggingface.co/collections/XiaomiMiMo/mimo-vl-68382ccacc7c2875500cd212) and [ModelScope Collections: MiMo-VL](https://www.modelscope.cn/collections/MiMo-VL-bb651017e02742)

|   **Model**    |                            **Description**                            |                           **Download (HuggingFace)**                            |                                 **Download (ModelScope)**                                 |
| :------------: | :-------------------------------------------------------------------: | :-----------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------: |
| MiMo-VL-7B-SFT | VLM with extraordinary reasoning potential after 4-stage pre-training | [ğŸ¤— XiaomiMiMo/MiMo-VL-7B-SFT](https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-SFT) | [ğŸ¤–ï¸ XiaomiMiMo/MiMo-VL-7B-SFT](https://www.modelscope.cn/models/XiaomiMiMo/MiMo-VL-7B-SFT) |
| MiMo-VL-7B-RL  |           RL model leapfrogging existing open-source models           |  [ğŸ¤— XiaomiMiMo/MiMo-VL-7B-RL](https://huggingface.co/XiaomiMiMo/MiMo-VL-7B-RL)  |  [ğŸ¤–ï¸ XiaomiMiMo/MiMo-VL-7B-RL](https://www.modelscope.cn/models/XiaomiMiMo/MiMo-VL-7B-RL)  |

## III. Evaluation Results

### General Capabilities

In general visual-language understanding, MiMo-VL-7B models achieve state-of-the-art open-source results.

<p align="center">
  <img width="95%" src="https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/benchmarks_general.png?raw=true">
</p>

### Reasoning Tasks

In multi-modal reasoning, both the SFT and RL models significantly outperform all compared open-source baselines across these benchmarks.

<p align="center">
  <img width="95%" src="https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/benchmarks_reasoning.png?raw=true">
</p>

> [!IMPORTANT]
> Results marked with \* are obtained using our evaluation framework.
> Tasks with ${\dagger}$ are evaluated by GPT-4o.

### GUI Tasks

MiMo-VL-7B-RL possess exceptional GUI understanding and grounding capabilities. As a general-purpose VL model, MiMo-VL achieves comparable or even superior performance to GUI-specialized models.

<p align="center">
  <img width="95%" src="https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/benchmarks_gui.png?raw=true">
</p>

### Elo Rating

With our in-house evaluation dataset and GPT-4o judgments, MiMo-VL-7B-RL achieves the highest Elo rating among all evaluated open-source vision-language models, ranking first across models spanning from 7B to 72B parameters.

<p align="center">
  <img width="95%" src="https://github.com/XiaomiMiMo/MiMo-VL/raw/main/figures/benchmarks_elo.png?raw=true">
</p>

### Evaluation Framework

To facilitate open research, we open-source our evaluation framework with all prompts used in [https://github.com/XiaomiMiMo/lmms-eval](https://github.com/XiaomiMiMo/lmms-eval).

## IV. Deployment

The MiMo-VL-7B series maintain full compatibility with the `Qwen2_5_VLForConditionalGeneration` architecture for deployment and inference.

## V. Citation

```bibtex
@misc{coreteam2025mimovltechnicalreport,
      title={MiMo-VL Technical Report}, 
      author={LLM-Core-Team Xiaomi},
      year={2025},
      eprint={2506.03569},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.03569}, 
}
```


## VI. Contact

Please contact us at [mimo@xiaomi.com](mailto:mimo@xiaomi.com) or open an issue if you have any questions.
